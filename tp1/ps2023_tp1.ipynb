{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ps2023-tp1",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smduarte/ps2022/blob/main/tp1/ps2022_tp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Processamento de Streams 2022\n",
        "## TP1 - Air Quality Monitoring (airborne particulate matter)\n",
        "-- version April 6 \n",
        " - updated to full dataset.\n",
        "\n",
        "-- version April 8 \n",
        " - added code for spark streaming (unstructured)\n",
        "\n",
        "-- version April 12\n",
        " - added a note to highlight the unstructured data\n",
        " format has the timestamp at the last position.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to analyze data provided by a set of air quality sensors [sds011](https://aqicn.org/sensor/sds011/pt/). The sensors present in the dataset are located in Portugal, namely in the Lisbon metro area. Each sensor provides two values: measuring particles less than 10 µm (P1) and less than 2.5 µm (P2) in μg/m³.\n",
        "\n",
        "The sensor data, spans the first half of 2020, and is streamed of Kafka. \n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | timestamp | P1 | P2\n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| timestamp | float | float\n",
        "\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "\n",
        "1. Find the time of day with the poorest air quality, for each location. Updated daily;\n",
        "2. Find the average air quality, for each location. Updated hourly;\n",
        "3. Can you show any daily and/or weekly patterns to air quality?;\n",
        "4. The data covers a period of extensive population confinement due to Covid 19. Can you find a signal in the data showing air quality improvement coinciding with the confinement period?"
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "1. Solve each question using one of the systems studied in the course.\n",
        "2. For questions not fully specified, provide your own interpretation, given your own analysis of the data."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading Criteria \n",
        "\n",
        "1. Bonus marks will be given for solving questions using more than one system (eg. Spark Unstructured + Spark Structured);\n",
        "2. Bonus marks will be given if some kind of graphical output is provided to present the results;\n",
        "3. Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "\n",
        "24th April + 1 day - ***no penalty***\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until\n",
        "the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eIWfDqQ3Cqi"
      },
      "source": [
        "#@title Mount Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.1.0\n",
        "KAFKA=kafka_2.13-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2022/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties\n"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Air quality sensor data publisher\n",
        "This a small python Kafka client that publishes a continous stream of text lines, obtained from the periodic output of the sensors.\n",
        "\n",
        "* The Kafka server is accessible @localhost:9092 \n",
        "* The events are published to the `air_quality` topic\n",
        "* Events are published 3600x faster than realtime relative to the timestamp\n"
      ],
      "metadata": {
        "id": "51ECJ--i0D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2022/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "\n",
        "cd kafka-tp1-logsender\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --topic air_quality  --speedup 3600 2> publisher-error.log > publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The python code below shows the basics needed to process JSON data from Kafka source using PySpark.\n",
        "\n",
        "Spark Streaming python documentation is found [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html)\n",
        "\n",
        "---\n",
        "#### PySpark Kafka Stream Example\n"
      ],
      "metadata": {
        "id": "1wihC26vaiT1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpO0aX2PPWd1"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df.show(20, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True),\n",
        "                     StructField('p2', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "query = lines \\\n",
        "    .writeStream \\\n",
        "    .outputMode('append') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Streaming (UnStructured) \n",
        "\n",
        "Latest Spark does not support Kafka sources with UnStructured Streaming.\n",
        "\n",
        "The next cell publishes the dataset using a TCP server, running at port 7777. For this mode, there is no need to install or run Kafka, using the cell above.\n",
        "\n",
        "The events are played faster than \"realtime\", at a 3600x speedup, such that 1 hour in terms of dataset timestamps is\n",
        "sent in 1 second realtime, provided the machine is fast enough. As such, Spark Streaming window functions need to be sized accordingly, since a minibatch of 1 second will be\n",
        "worth 1 hour of dataset events."
      ],
      "metadata": {
        "id": "EMAyVFCwTTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Socket-based Publisher\n",
        "%%bash\n",
        "\n",
        "git clone https://github.com/smduarte/ps2022.git 2> /dev/null > /dev/null || git -C ps2022 pull\n",
        "cd ps2022/colab/socket-tp1-logsender/\n",
        "\n",
        "nohup python publisher.py --filename 2020-01-06_sds011-pt.csv --port 7777  --speedup 3600 2> /tmp/publisher-error.log > /tmp/publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each line sample has the following parts separated by blanks:\n",
        "\n",
        "sensor_id | sensor_type | location | latitude | longitude | P1 | P2 | timestamp \n",
        "----------|-------------|----------|----------|-----------|-----------|----|---\n",
        "string  | string | string | float | float| float | float | timestamp\n",
        "\n"
      ],
      "metadata": {
        "id": "DTogabrlXZSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Spark Streaming (Unstructured) Example\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777) \n",
        "\n",
        "  lines.pprint()\n",
        "    \n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(20)  \n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "RLHQoF9xOLKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
